---
title: "When Does Student's t Become Normal? A Simulation"
output:
  md_document:
    variant: gfm
    preserve_yaml: TRUE
author: "steve"
date: '2022-01-27'
excerpt: "The t-distribution is the sister to the normal distribution, and it doesn't take too many degrees of freedom for the two to become indistinguishable."
layout: post
categories:
  - R
image: "guinness.jpg"
---

```{r setup, include=FALSE, cache=F}

rmd_name <- knitr::current_input()
rmd_name <- stringr::str_sub(rmd_name, 12, -1)
rmd_name <- stringr::str_sub(rmd_name, 1, stringr::str_length(rmd_name)-4)


base_dir <- "~/Dropbox/svmiller.github.io/"
base_url <- "/"
fig_path <- paste0("images/", rmd_name, "/")

cache_path <- paste0("~/Dropbox/svmiller.github.io/cache/", rmd_name, "/")

add_jekyll_image <- function(url, caption, width, align) {
 img <- paste0('{% include image.html url="',url,'" caption="',caption,'" width=',width,' align="',align,'" %}')
 cat(img)
}

add_update <- function(announce, text) {
  
  update <- paste0('{% include updatebox.html announce="',announce,'" text="',text,'" %}')
 cat(update)
  
}

knitr::opts_knit$set(base.dir = base_dir, base.url = base_url)
knitr::opts_chunk$set(fig.path = fig_path, dpi= 300,
                      cache.path = cache_path,
                      message=FALSE, warning=FALSE,
                      cache = FALSE,
                      collapse = TRUE, comment = "#>") 

library(tidyverse)     # for most things
library(stevemisc)     # for graph formatting
library(kableExtra)

Sims <- readRDS(url("http://svmiller.com/R/t-normal/t-normal-sims.rds"))
```

```{r leadimage, echo=F, eval=T, results="asis", cache=F}
 
add_jekyll_image('/images/guinness.jpg', "I'd be lying if I said I cared for Guinness' stouts, but the brewery does have a special place in the history of applied statistics.", "350", "right")
 
```

The premise for this post started as a question a student raised the last time I taught [my undergrad quantitative methods class](http://posc3410.svmiller.com/). In lecture, I referenced Student's *t*-distribution as the appropriate distribution for inferring from a sample statistic to a population in situations where we know neither $$\sigma$$ or $$\mu$$ and still want to proceed with inference as I teach them with a simulation where we do happen to know $$\mu$$ and $$\sigma$$. The basic point is to plug in the sample statistics (mean and standard deviation), calculate a *t*-value, and find the appropriate test statistic from the table of probabilities associated with some critical value of interest (typically *p < .05*). The process of inference is the same, albeit with a different statistic and table, with the assurance that Student's *t*-distribution collapses to the normal distribution with more degrees of freedom. The off-hand comment I made was it collapses to the normal distribution "pretty quickly."

A student asked "how quickly", and I didn't have much of an answer other than "pretty quickly", and certainly before you get to the last row of the table we all get where the degrees of freedom is 1,000. Here, I decided to see exactly where I could tell a student of mine that Student's *t*-distribution becomes indistinguishable from the normal distribution.

## The Setup

A situation like this calls for keeping as many things standardized as possible, so the setup here will compare the "standard normal distribution" (where $$\mu$$ is 0 and $$\sigma^2$$ is 1 [and $$\sigma$$ is 1 as well]) as a referent distribution to a Student's *t*-distribution with the same parameters. The Student's *t*-distribution will have varying degrees of freedom from 1 to 100, and, to boot, 1,000. Most *t*-distribution tables I see have these varying degrees of freedom (i.e. 1 to 100) and, at the end, a large one (like 1,000) that shows the differences in probabilities with what might otherwise be gathered from the standard normal distribution are mostly at the thousandths of a decimal point. The distributions will further be simulated with varying number of observations (*n* = 10, 25, 50, 100, 400, 1000, 3000), each setup simulated 1,000 times. Doing the math, this is 1,000 simulations of seven different hypothetical sample sizes with 101 alternate degrees of freedom. That's 707,000 different simulations. 

Each of these 707,000 simulations will end in three different tests: the [Kolmogorovâ€“Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test), the [Anderson-Darling test](https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test), and the [Shapiro-Wilk test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test). These are three various tests of the "normality" of the data fed to it. Two of them (Kolmogorov-Smirnov and Shapiro-Wilk) are often taught as default tests of normality. The Anderson-Darling test isn't taught as much, but perhaps it should. At least as I understand it, it's more robust than the Kolmogorov-Smirnov test. No matter, the test statistics from all these tests are set against a null hypothesis that the data observed could have been plausibly generated from a normal distribution. A test statistic with a small enough *p*-value---whatever you choose---suggests rejecting that null hypothesis and asserting instead that the data were not drawn from a normal distribution.

This is going to be a huge simulation, so leveraging multiple cores is ideal here. It's also a great opportunity to extend how I've been using the `{foreach}` package of late. All told, the script I set up look like this.

```r
library(stevemisc) # for simulating from Student's t
library(tidyverse) # for most things
library(foreach)   # for some parallel magic
library(nortest)   # for the Anderson-Darling test, which isn't in base R

# use half available cores, and set them up
half_cores  <- parallel::detectCores()/2
library(foreach)
my.cluster <- parallel::makeCluster(
  half_cores,
  type = "PSOCK"
)

doParallel::registerDoParallel(cl = half_cores)
foreach::getDoParRegistered()

# reproducible seed
set.seed(8675309)
Sims <- foreach(
  # for 1,000 simulations....
  i = 1:1000,
  .combine = 'rbind'
) %:%
  # for these varying number of observations...
  foreach(nobs = c(10, 25, 50, 100, 400, 1000, 3000),
          .combine = 'rbind') %:%
  # for these varying degrees of freedom...
  foreach(dfs = c(1:100, 1000),
          .combine = 'rbind') %dopar% { # do all this stuff
    hold_this <- rst(nobs, dfs, 0, 1)
    ks <- broom::tidy(ks.test(hold_this, "pnorm", 0, 1)) %>% mutate(n = nobs, df = dfs, method = "K-S")
    sw <- broom::tidy(shapiro.test(hold_this)) %>% mutate(n = nobs, df = dfs, method = "S-W")
    ad <- broom::tidy(ad.test(hold_this)) %>% mutate(n = nobs, df = dfs, method = "A-D")
    binded <- bind_rows(ks, sw, ad) %>% select(-alternative) %>% mutate(iter = i)
  } 
  
# close our clusters...
parallel::stopCluster(cl = my.cluster) 
```

The end result here, after some time, is a data frame of 2,121,000 rows summarizing these three tests across these 707,000 combinations.


```{r}
Sims
```

What is a "significant" result against the null (i.e. the data were likely not drawn from a normal distribution) is arbitrary, and to the user's discretion. No matter, everyone loves *p < .05*, so we'll employ that criteria. Where *p < .05*, the simulation suggests the data were not drawn from a normal distribution.


```{r}
Sims %>% 
  mutate(not_normal = ifelse(p.value < .05, 1, 0)) -> Sims
```

Here are some basic takeaways from these simulations.

## The Kolmogorov-Smirnov Test Struggles to Reject the Null

One thing that will stand out immediately is that the  Kolmogorov-Smirnov test is going to struggle to reject the null hypothesis more than the other tests. I'm fairly sure this is common knowledge in the stats world. The advantage of the Kolmogorov-Smirnov test is that it's more general, but less discerning than it should be. Consider that the true state of the world is that none of these samples were generated from a normal distribution. In an ideal application, all three tests should reject the null hypothesis 100% of the time no matter the degrees of freedom and sample size. However, these are the proportions of null rejections by each method.

```{r}
Sims %>% group_by(method) %>% summarize(meand = mean(not_normal))
```

The Kolmogorov-Smirnov test rejects the null only about 7.5% of the time across all 707,000 combinations of sample size and degrees of freedom. By comparison, the Shapiro-Wilk test rejects the null about 17% of the time and the Anderson-Darling test rejects the null about 14.2% of the time. I suppose this squares with the methods training I got that elevated the Shapiro-Wilk test over the Kolmogorov-Smirnov test and treats the Anderson-Darling test as a "more robust" alternative to the Kolmogorov-Smirnov test.

The Kolmogorov-Smirnov test particularly struggles when it doesn't have a whole lot of observations in the sample. Consider the graph below. Here, I select on just those simulations with 10 degrees of freedom or fewer, varying by the seven sample sizes I employ. In theory, these degrees of freedom should combine to be a relatively simple case of a null rejection for all three tests. Even with 10 degrees of freedom, randomly simulating from a Student's *t*-distribution with a mean of 0 and a standard deviation of 1 will still produce (absolute) values greater than 2. A test for normality should identify these as constituting fatter tails away from the mean, and fatter relative to the standard normal distribution. This should be easy for all three tests to discern.

```{r comparing-normality-tests-10-or-fewer-dfs, echo=F, eval=T, fig.width=11, fig.height =8}
Sims %>% filter(df <= 10) %>% 
  group_by(n, df, method) %>% 
  summarize(prop_nn = mean(not_normal)) %>%
  mutate(n = paste("Sample Size:", n),
         n = fct_inorder(n),
         method = recode(method,
                         "A-D" = "Anderson-Darling",
                         "K-S" = "Kolmogorov-Smirnov",
                         "S-W" = "Shapiro-Wilk")) %>%
  ggplot(.,aes(as.factor(df), prop_nn, fill=method)) + 
  geom_bar(stat="identity", position="dodge", color="black") +
  geom_hline(yintercept = 0.05, linetype="dashed")+
  facet_wrap(~n) +
  scale_fill_brewer(palette="Set2") +
  theme_steve_web() + post_bg() +
  scale_y_continuous(labels = scales::percent) +
  labs(fill = "", x = "Degrees of Freedom",
       y = "Proportion of Null Rejections",
       title = "A Comparison of Three Tests for Normality With 10 or Fewer Degrees of Freedom",
       subtitle = "All tests struggle with few sample observations, but the Kolmogorov-Smirnov struggles more than the two other alternatives.",
       caption = "Horizontal line drawn at .05 (i.e. 5%) for each facet.")
```

However, all tests will struggle with a smaller sample size and these few degrees of freedom and the Kolmogorov-Smirnov test will struggle relative to the Anderson-Darling and Shapiro-Wilk tests. Where the sample size is 10 and the degrees of freedom are 1, 69.9% of the 1,000 data simulations produce Anderson-Darling tests that reject the null hypothesis of normality. 58.6% of the Shapiro-Wilk tests will reject the null hypothesis of normality. Only about 11% of the Kolmogorov-Smirnov tests will reject the null hypothesis of normality. All tests will struggle with a smaller sample size; the Kolmogorov-Smirnov test will struggle more than the other two. Larger sample sizes help the Kolmogorov-Smirnov test reject the null, but the drop-off in accuracy is still evident. With a sample size of 3,000 and 10 degrees of freedom, 99.9% of the Anderson-Darling tests still reject the null. 100% of the Shapiro-Wilk tests rejected the nully. Only 29.6% of the Kolmogorov-Smirnov tests rejected the null.

I mention this to say that, the extent to which this could also double as a simulation of how well each tests does in identifying data not drawn from a normal distribution, this could also be offered as evidence for thinking about Smirnov-Wilk or Anderson-Darling as default tests for normality. The Kolmogorov-Smirnov test is not as robust as these two, and will struggle more than the others with smaller sample sizes.


```{r comparing-normality-tests-100-or-fewer-dfs, echo=F, eval=T, fig.width=11, fig.height =8}
Sims %>% 
    group_by(n, df, method) %>% 
    summarize(prop_nn = mean(not_normal)) %>%
    mutate(n = paste("Sample Size:", n),
           n = fct_inorder(n),
           method = recode(method,
                           "A-D" = "Anderson-Darling",
                           "K-S" = "Kolmogorov-Smirnov",
                           "S-W" = "Shapiro-Wilk")) %>% 
  ungroup() %>% filter(df < 1000) %>%
  ggplot(.,aes(df, prop_nn, color=method)) + 
  #geom_bar(stat="identity", position="dodge", color="black") +
  geom_line() +
  geom_hline(yintercept = 0.05, linetype="dashed") +
  facet_wrap(~n) +
  scale_color_brewer(palette="Set2") +
  theme_steve_web() + post_bg() +
  scale_y_continuous(labels = scales::percent) +
  labs(fill = "", x = "Degrees of Freedom",
       y = "Proportion of Null Rejections",
       title = "A Comparison of Three Tests for Normality With 100 or Fewer Degrees of Freedom",
       subtitle = "I'm working on it.",
       caption = "Horizontal line drawn at .05 (i.e. 5%) for each facet.")
```